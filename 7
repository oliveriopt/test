from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from google.cloud import bigquery
from datetime import datetime, timedelta


default_args = {
    'owner': 'dataops',
    'start_date': datetime(2025, 7, 24),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

PROJECT_ID = "rxo-dataeng-datalake-np"
QUERY_PLAN_TABLE = "dataops_admin.qa_query_plan"
GCS_TEMPLATE_PATH = "gs://rxo-dataeng-datalake-np-dataflow/templates/sql-parquet-to-gcs-batched-ap.json"
TEMP_BUCKET = "rxo-dataeng-datalake-np-dataflow"
OUTPUT_GCS_PREFIX = f"gs://{TEMP_BUCKET}/qa-results"

def get_queries_for_table(table_name: str):
    client = bigquery.Client(project=PROJECT_ID)
    query = f"""
        SELECT table_name, check_type, query_text
        FROM `{PROJECT_ID}.{QUERY_PLAN_TABLE}`
        WHERE table_name = '{table_name}'
    """
    df = client.query(query).to_dataframe(create_bqstorage_client=False)
    return df.to_dict(orient="records")


with DAG(
    dag_id="run_qa_queries_flex_template_single_table",
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    params={"table_name": "ventas.clientes"},  # 🔁 Puedes cambiar aquí el valor por defecto
    tags=["qa", "dataflow"],
) as dag:

    def fetch_queries(**context):
        table_name = context["params"]["table_name"]
        return get_queries_for_table(table_name)

    load_queries = PythonOperator(
        task_id="load_queries_for_table",
        python_callable=fetch_queries
    )

    def run_dataflow(**context):
        from airflow.models import Variable
        ti = context['ti']
        queries = ti.xcom_pull(task_ids="load_queries_for_table")

        tasks = []
        for i, row in enumerate(queries):
            table = row["table_name"]
            check = row["check_type"]
            query = row["query_text"]

            schema, table_only = table.split(".")
            output_path = f"{OUTPUT_GCS_PREFIX}/table_name={table}/check_type={check}/run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.parquet"

            task = DataflowStartFlexTemplateOperator(
                task_id=f"run_dataflow_{i}_{check}",
                project_id=PROJECT_ID,
                location="us-central1",
                body={
                    "launchParameter": {
                        "jobName": f"qa-{check.replace('_','')}-{i}",
                        "containerSpecGcsPath": GCS_TEMPLATE_PATH,
                        "parameters": {
                            "gcp_project": PROJECT_ID,
                            "schema": schema,
                            "table": table_only,
                            "query": query,
                            "output_gcs_path": output_path,
                            "reload_flag": "true",
                            "type_of_extraction": "custom",
                            "primary_key": "ID",  # Ajustar si es necesario
                            "batch_size": "100000",
                            "chunk_size": "100000",
                            "secret_id": "sqlserver-conn-secret"
                        },
                        "environment": {
                            "tempLocation": f"gs://{TEMP_BUCKET}/temp",
                            "serviceAccountEmail": "ds-dataflow-dataeng-gsa@rxo-dataeng-datalake-np.iam.gserviceaccount.com",
                            "subnetwork": "https://www.googleapis.com/compute/v1/projects/nxo-corp-infra/regions/us-central1/subnetworks/rxo-dataeng-datalake-np-uscentral1"
                        }
                    }
                }
            )
            task.dag = dag
            tasks.append(task)

        return tasks

    run_dataflow_tasks = PythonOperator(
        task_id="launch_dataflow_flex_jobs",
        python_callable=run_dataflow
    )

    load_queries >> run_dataflow_tasks
