from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.utils.dates import days_ago
from datetime import datetime
import os
import yaml
import uuid

# === Config ===
PROJECT_ID = "rxo-dataeng-datalake-np"
REGION = "us-central1"
SERVICE_ACCOUNT = "ds-dataflow-dataeng-gsa@rxo-dataeng-datalake-np.iam.gserviceaccount.com"
SUBNETWORK = "https://www.googleapis.com/compute/v1/projects/nxo-corp-infra/regions/us-central1/subnetworks/rxo-dataeng-datalake-np-uscentral1"
TEMPLATE_GCS_PATH = "gs://rxo-dataeng-datalake-np-dataflow/templates/sql_to_parquet.json"
TEMP_LOCATION = "gs://dataflow-staging-us-central1-387408803089/temp_files"
STAGING_LOCATION = "gs://dataflow-staging-us-central1-387408803089/staging_area"

YAML_PATH = os.path.join(os.path.dirname(__file__), "include", "qa_config.yaml")

def read_config_task():
    with open(YAML_PATH, "r") as f:
        config = yaml.safe_load(f)

    required_keys = {"table_catalog", "table_schema", "table_name"}
    if not required_keys.issubset(config):
        raise ValueError(f"Missing keys in YAML config: {required_keys}")

    return config

def get_queries_from_bq(config):
    catalog = config["table_catalog"]
    schema = config["table_schema"]
    table = config["table_name"]

    query = f"""
        SELECT query_text
        FROM `rxo-dataeng-datalake-np.dataops_admin.qa_query_plan`
        WHERE table_catalog = '{catalog}'
          AND table_schema = '{schema}'
          AND table_name = '{table}'
    """

    hook = BigQueryHook(gcp_conn_id="google_cloud_default", use_legacy_sql=False)
    df = hook.get_pandas_df(sql=query, location=REGION)

    if df.empty:
        raise ValueError(f"No query_text found for {catalog}.{schema}.{table}")

    return df["query_text"].tolist()

def get_secret_id_from_bq(config):
    catalog = config["table_catalog"]
    schema = config["table_schema"]
    table = config["table_name"]

    query = f"""
        SELECT secret_id
        FROM `rxo-dataeng-datalake-np.dataops_admin.table_extraction_metadata`
        WHERE database_name = '{catalog}'
          AND schema_name = '{schema}'
          AND table_name = '{table}'
        LIMIT 1
    """

    hook = BigQueryHook(gcp_conn_id="google_cloud_default", use_legacy_sql=False)
    df = hook.get_pandas_df(sql=query, location=REGION)

    if df.empty:
        raise ValueError(f"No secret_id found for {catalog}.{schema}.{table}")

    return df["secret_id"].iloc[0]

def build_and_run_flex(**kwargs):
    from airflow.operators.python import get_current_context
    context = get_current_context()
    query_text = context["query_text"]
    config = context["config"]
    secret_id = context["secret_id"]

    catalog = config["table_catalog"]
    schema = config["table_schema"]
    table = config["table_name"]
    today = datetime.utcnow()

    output_path = (
        f"gs://rxo-dataeng-datalake-np-raw/qa/{catalog}/{schema}/{table}/"
        f"{today.year:04d}/{today.month:02d}/{today.day:02d}/"
        f"data-{uuid.uuid4().hex[:8]}.parquet"
    )

    return {
        "launchParameter": {
            "jobName": f"sqlserver-to-gcs-{table.lower()}-{uuid.uuid4().hex[:4]}",
            "containerSpecGcsPath": TEMPLATE_GCS_PATH,
            "environment": {
                "serviceAccountEmail": SERVICE_ACCOUNT,
                "subnetwork": SUBNETWORK,
                "tempLocation": TEMP_LOCATION,
                "stagingLocation": STAGING_LOCATION,
                "ipConfiguration": "WORKER_IP_PRIVATE"
            },
            "parameters": {
                "query": query_text,
                "secret_id": secret_id,
                "gcp_pr": PROJECT_ID,
                "output_path": output_path
            }
        }
    }

with DAG(
    dag_id="dag_qa_pipeline_parallel",
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=["qa", "dataflow", "parallel"]
) as dag:

    def orchestrate_tasks(**kwargs):
        config = read_config_task()
        queries = get_queries_from_bq(config)
        secret_id = get_secret_id_from_bq(config)

        tasks = []
        for idx, query_text in enumerate(queries):
            task_id = f"run_flex_template_{idx}"

            flex_body = build_and_run_flex(
                config=config,
                query_text=query_text,
                secret_id=secret_id
            )

            run_flex = DataflowStartFlexTemplateOperator(
                task_id=task_id,
                project_id=PROJECT_ID,
                location=REGION,
                body=flex_body,
                gcp_conn_id="google_cloud_default",
            )
            tasks.append(run_flex)
        return tasks

    launch_all = PythonOperator(
        task_id="launch_all_flex_templates",
        python_callable=orchestrate_tasks
    )
