from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

import yaml
import os

# ConfiguraciÃ³n general
PROJECT_ID = "rxo-dataeng-datalake-np"
REGION = "us-central1"
SERVICE_ACCOUNT = "ds-dataflow-dataeng-gsa@rxo-dataeng-datalake-np.iam.gserviceaccount.com"
SUBNETWORK = "https://www.googleapis.com/compute/v1/projects/nxo-corp-infra/regions/us-central1/subnetworks/rxo-dataeng-datalake-np-uscentral1"
TEMPLATE_GCS_PATH = "gs://rxo-dataeng-datalake-np-dataflow/templates/sql_to_parquet.json"

CONFIG_PATH = os.path.join(os.path.dirname(__file__), "../include/qa_config.yaml")

with DAG(
    dag_id="dag_qa_pipeline_parallel",
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=["qa", "parallel", "dataflow"],
) as dag:

    @task(task_id="read_config", multiple_outputs=True)
    def read_config():
        with open(CONFIG_PATH, "r") as file:
            config = yaml.safe_load(file)
        return {
            "catalog": config["table_catalog"],
            "schema": config["table_schema"],
            "table": config["table_name"],
        }

    @task(task_id="get_secret_id")
    def get_secret_id(catalog: str, schema: str, table: str) -> str:
        query = f"""
        SELECT secret_id
        FROM `rxo-dataeng-datalake-np.dataops_admin.qa_query_plan`
        WHERE table_catalog = '{catalog}'
          AND table_schema = '{schema}'
          AND table_name = '{table}'
        LIMIT 1
        """
        hook = BigQueryHook(gcp_conn_id="google_cloud_default", use_legacy_sql=False)
        df = hook.get_pandas_df(query)
        return df["secret_id"].iloc[0]

    @task(task_id="get_queries", multiple_outputs=True)
    def get_queries(catalog: str, schema: str, table: str) -> dict:
        query = f"""
        SELECT query_text
        FROM `rxo-dataeng-datalake-np.dataops_admin.qa_query_plan`
        WHERE table_catalog = '{catalog}'
          AND table_schema = '{schema}'
          AND table_name = '{table}'
        LIMIT 5
        """
        hook = BigQueryHook(gcp_conn_id="google_cloud_default", use_legacy_sql=False)
        df = hook.get_pandas_df(query)
        return {f"query_{i}": row for i, row in enumerate(df["query_text"].tolist())}

    config = read_config()
    secret_id = get_secret_id(config["catalog"], config["schema"], config["table"])
    queries = get_queries(config["catalog"], config["schema"], config["table"])

    for i in range(5):
        DataflowStartFlexTemplateOperator(
            task_id=f"run_flex_template_{i}",
            project_id=PROJECT_ID,
            location=REGION,
            body={
                "launchParameter": {
                    "jobName": f"qa-flex-job-{i}",
                    "containerSpecGcsPath": TEMPLATE_GCS_PATH,
                    "environment": {
                        "serviceAccountEmail": SERVICE_ACCOUNT,
                        "subnetwork": SUBNETWORK,
                        "tempLocation": "gs://dataflow-staging-us-central1-387408803089/temp_files",
                        "stagingLocation": "gs://dataflow-staging-us-central1-387408803089/staging_area",
                        "ipConfiguration": "WORKER_IP_PRIVATE",
                    },
                    "parameters": {
                        "query": f"{{{{ ti.xcom_pull(task_ids='get_queries')['query_{i}'] }}}}",
                        "output_path": f"gs://rxo-dataeng-datalake-np-raw/qa_output/query_{i}.snappy.parquet",
                        "gcp_pr": PROJECT_ID,
                        "secret_id": f"{{{{ ti.xcom_pull(task_ids='get_secret_id') }}}}",
                    },
                }
            },
            gcp_conn_id="google_cloud_default",
        )
