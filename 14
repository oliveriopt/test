from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule
import yaml
import os
from datetime import datetime
import uuid

# === Config ===
PROJECT_ID = "rxo-dataeng-datalake-np"
REGION = "us-central1"
SERVICE_ACCOUNT = "ds-dataflow-dataeng-gsa@rxo-dataeng-datalake-np.iam.gserviceaccount.com"
SUBNETWORK = "https://www.googleapis.com/compute/v1/projects/nxo-corp-infra/regions/us-central1/subnetworks/rxo-dataeng-datalake-np-uscentral1"
TEMPLATE_GCS_PATH = "gs://rxo-dataeng-datalake-np-dataflow/templates/sql_to_parquet.json"
YAML_PATH = os.path.join(os.path.dirname(__file__), "include", "qa_config.yaml")

# === Funciones ===
def read_config(**kwargs):
    with open(YAML_PATH, "r") as f:
        config = yaml.safe_load(f)
    kwargs["ti"].xcom_push(key="config", value=config)

def get_queries(**kwargs):
    ti = kwargs["ti"]
    config = ti.xcom_pull(task_ids="read_config", key="config")
    catalog, schema, table = config["table_catalog"], config["table_schema"], config["table_name"]

    query = f"""
        SELECT query_text FROM `rxo-dataeng-datalake-np.dataops_admin.qa_query_plan`
        WHERE table_catalog = '{catalog}' AND table_schema = '{schema}' AND table_name = '{table}'
        LIMIT 5
    """
    hook = BigQueryHook()
    df = hook.get_pandas_df(query)
    queries = df["query_text"].tolist()
    ti.xcom_push(key="queries", value=queries)

def get_secret_id(**kwargs):
    ti = kwargs["ti"]
    config = ti.xcom_pull(task_ids="read_config", key="config")
    catalog, schema, table = config["table_catalog"], config["table_schema"], config["table_name"]

    query = f"""
        SELECT secret_id FROM `rxo-dataeng-datalake-np.dataops_admin.table_extraction_metadata`
        WHERE database_name = '{catalog}' AND schema_name = '{schema}' AND table_name = '{table}'
        LIMIT 1
    """
    hook = BigQueryHook()
    df = hook.get_pandas_df(query)
    secret_id = df["secret_id"].iloc[0]
    ti.xcom_push(key="secret_id", value=secret_id)

def build_body(**kwargs):
    ti = kwargs["ti"]
    query_text = kwargs["query"]
    index = kwargs["index"]
    config = ti.xcom_pull(task_ids="read_config", key="config")
    secret_id = ti.xcom_pull(task_ids="get_secret_id", key="secret_id")
    catalog, schema, table = config["table_catalog"], config["table_schema"], config["table_name"]
    today = datetime.utcnow()

    output_path = (
        f"gs://rxo-dataeng-datalake-np-raw/qa/{catalog}/{schema}/{table}/"
        f"{today.year:04d}/{today.month:02d}/{today.day:02d}/data-{uuid.uuid4().hex[:8]}.parquet"
    )

    body = {
        "launchParameter": {
            "jobName": f"sqlserver-to-gcs-{table.lower()}-{index}-{today.strftime('%Y%m%d%H%M%S')}",
            "containerSpecGcsPath": TEMPLATE_GCS_PATH,
            "environment": {
                "serviceAccountEmail": SERVICE_ACCOUNT,
                "subnetwork": SUBNETWORK,
                "tempLocation": "gs://dataflow-staging-us-central1-387408803089/temp_files",
                "stagingLocation": "gs://dataflow-staging-us-central1-387408803089/staging_area",
                "ipConfiguration": "WORKER_IP_PRIVATE",
            },
            "parameters": {
                "query": query_text,
                "secret_id": secret_id,
                "gcp_pr": PROJECT_ID,
                "output_path": output_path,
            },
        }
    }
    ti.xcom_push(key=f"flex_body_{index}", value=body)

# === DAG ===
def build_parallel_pipeline():
    with DAG(
        dag_id="dag_qa_pipeline_parallel",
        start_date=days_ago(1),
        schedule_interval=None,
        catchup=False,
        tags=["qa", "flex", "parallel"]
    ) as dag:

        read_config_task = PythonOperator(
            task_id="read_config",
            python_callable=read_config,
        )

        get_queries_task = PythonOperator(
            task_id="get_queries",
            python_callable=get_queries,
        )

        get_secret_id_task = PythonOperator(
            task_id="get_secret_id",
            python_callable=get_secret_id,
        )

        read_config_task >> [get_queries_task, get_secret_id_task]

        # Crear dinÃ¡micamente 5 pipelines
        for i in range(5):
            def _build_body_callable(index=i):
                return PythonOperator(
                    task_id=f"build_body_{index}",
                    python_callable=build_body,
                    op_kwargs={"index": index, "query": f"{{{{ ti.xcom_pull(task_ids='get_queries', key='queries')[{index}] }}}}"},
                )

            build_body_task = _build_body_callable()

            run_flex_task = DataflowStartFlexTemplateOperator(
                task_id=f"run_flex_{i}",
                project_id=PROJECT_ID,
                location=REGION,
                gcp_conn_id="google_cloud_default",
                body=f"{{{{ ti.xcom_pull(task_ids='build_body_{i}', key='flex_body_{i}') }}}}"
            )

            [get_queries_task, get_secret_id_task] >> build_body_task >> run_flex_task

    return dag

dag = build_parallel_pipeline()
