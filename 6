from airflow import DAG
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from airflow.operators.python import PythonOperator
from google.cloud import bigquery
from datetime import datetime, timedelta


default_args = {
    'owner': 'dataops',
    'start_date': datetime(2025, 7, 24),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

PROJECT_ID = "rxo-dataeng-datalake-np"
QUERY_PLAN_TABLE = "dataops_admin.qa_query_plan"
GCS_TEMPLATE_PATH = "gs://rxo-dataeng-datalake-np-dataflow/templates/sql-parquet-to-gcs-batched-ap.json"
TEMP_BUCKET = "rxo-dataeng-datalake-np-dataflow"
OUTPUT_GCS_PREFIX = f"gs://{TEMP_BUCKET}/qa-results"

def get_query_plan():
    client = bigquery.Client(project=PROJECT_ID)
    query = f"SELECT table_name, check_type, query_text FROM `{PROJECT_ID}.{QUERY_PLAN_TABLE}`"
    df = client.query(query).to_dataframe(create_bqstorage_client=False)
    return df.to_dict(orient="records")


with DAG(
    dag_id="run_qa_queries_flex_template",
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=["qa", "dataflow"],
) as dag:

    def fetch_queries(**context):
        return get_query_plan()

    load_query_list = PythonOperator(
        task_id="load_query_plan",
        python_callable=fetch_queries,
    )

    def build_dataflow_task(query_entry: dict, index: int):
        table = query_entry["table_name"]
        check = query_entry["check_type"]
        query = query_entry["query_text"]
        output_path = f"{OUTPUT_GCS_PREFIX}/table_name={table}/check_type={check}/run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.parquet"

        return DataflowStartFlexTemplateOperator(
            task_id=f"run_dataflow_{index}_{check}",
            project_id=PROJECT_ID,
            location="us-central1",
            body={
                "launchParameter": {
                    "jobName": f"qa-{check.replace('_','')}-{index}",
                    "containerSpecGcsPath": GCS_TEMPLATE_PATH,
                    "parameters": {
                        "gcp_project": PROJECT_ID,
                        "schema": table.split('.')[0],
                        "table": table.split('.')[1],
                        "query": query,
                        "output_gcs_path": output_path,
                        "reload_flag": "true",
                        "type_of_extraction": "custom",
                        "primary_key": "ID",  # placeholder if required
                        "batch_size": "100000",
                        "chunk_size": "100000",
                        "secret_id": "sqlserver-conn-secret"
                    },
                    "environment": {
                        "tempLocation": f"gs://{TEMP_BUCKET}/temp",
                        "serviceAccountEmail": "ds-dataflow-dataeng-gsa@rxo-dataeng-datalake-np.iam.gserviceaccount.com",
                        "subnetwork": "https://www.googleapis.com/compute/v1/projects/nxo-corp-infra/regions/us-central1/subnetworks/rxo-dataeng-datalake-np-uscentral1"
                    }
                }
            }
        )

    from airflow.utils.task_group import TaskGroup

    with TaskGroup("run_queries") as run_queries:
        from airflow.operators.empty import EmptyOperator
        import uuid

        # En tiempo real, crear una lista dinámica de operadores no es trivial, así que solo ejemplo
        from airflow.operators.python import get_current_context

        def launch_dynamic_tasks(**context):
            ti = context['ti']
            queries = ti.xcom_pull(task_ids="load_query_plan")
            for i, row in enumerate(queries):
                t = build_dataflow_task(row, i)
                t.dag = dag  # necesario para registrar la tarea en Composer
                dag.add_task(t)

        trigger = PythonOperator(
            task_id="launch_dataflow_jobs",
            python_callable=launch_dynamic_tasks,
        )

    load_query_list >> run_queries
