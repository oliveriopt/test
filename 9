from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
import yaml

# === Configuración general del DAG ===
default_args = {
    "owner": "airflow",
    "retries": 1,
}

dag_config_path = "data/information_schema/orders_order.yaml"

# === Paso 1: Cargar configuración YAML ===
def load_config(**kwargs):
    with open(dag_config_path, "r") as f:
        config = yaml.safe_load(f)
    required_keys = {"server_name", "schema_name", "table_name"}
    if not required_keys.issubset(config):
        raise ValueError(f"Missing keys in YAML config. Required: {required_keys}")
    
    kwargs["ti"].xcom_push(key="config", value=config)

# === Paso 2: Crear tareas dinámicas para QA ===
def create_qa_tasks(**kwargs):
    from airflow.operators.python import PythonOperator

    ti = kwargs["ti"]
    dag = kwargs["dag"]
    config = ti.xcom_pull(key="config", task_ids="load_config")

    server = config["server_name"]
    schema = config["schema_name"]
    table = config["table_name"]

    query_sql = """
        SELECT check_type, query_text
        FROM `rxo-dataeng-datalake-np.dataops_admin.qa_query_plan`
        WHERE server_name = @server
          AND schema_name = @schema
          AND table_name = @table
    """

    bq_hook = BigQueryHook(gcp_conn_id="google_cloud_default", use_legacy_sql=False)
    job = bq_hook.run_query(
        sql=query_sql,
        location="us-central1",
        params={"server": server, "schema": schema, "table": table},
        param_types={"server": "STRING", "schema": "STRING", "table": "STRING"},
    )
    df = bq_hook.get_pandas_df(job)

    if df.empty:
        raise ValueError("No QA queries found")

    for _, row in df.iterrows():
        check_type = row["check_type"]
        query_text = row["query_text"]

        def execute_qa_query(query=query_text, check=check_type, **_):
            print(f"[QA - {check}] Ejecutando query:\n{query}")
            # Aquí puedes reemplazar por lógica real de ejecución (Dataflow, pyodbc, etc.)

        task = PythonOperator(
            task_id=f"qa_{check_type}",
            python_callable=execute_qa_query,
            provide_context=True,
            dag=dag,
        )
        dag.get_task("create_qa_tasks") >> task

# === Definición del DAG ===
with DAG(
    dag_id="dag_qa_sqlserver_table_parallel",
    default_args=default_args,
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=["qa", "dataflow", "sqlserver"],
) as dag:

    load_config_task = PythonOperator(
        task_id="load_config",
        python_callable=load_config,
        provide_context=True,
    )

    create_tasks = PythonOperator(
        task_id="create_qa_tasks",
        python_callable=create_qa_tasks,
        provide_context=True,
    )

    load_config_task >> create_tasks
