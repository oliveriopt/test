import sys
import logging
import yaml
import json
from datetime import datetime
from pathlib import Path
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.google.cloud.operators.dataflow import DataflowStartFlexTemplateOperator
from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
    BigQueryGetDataOperator,
)
from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.task_group import TaskGroup
from include.utils.pipe_config_loader import PipeConfigLoader  
from include.utils.build_iceberg_utils import build_iceberg_sql


# === Load configurations ===
config_loader = PipeConfigLoader()
pipe_params, env_config = config_loader.load_configurations("analytical_tables")

# === Start DAG definition ===
with DAG(
    dag_id="dag_sqlserver_to_gcs_parallel",
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=["parallel", "etl", "yaml"],
    max_active_runs=1
) as dag:

    # === Optional logging task ===
    log_start = PythonOperator(
        task_id="log_start",
        python_callable=lambda **context: print("DAG started")
    )

    task_groups = []

    for table in pipe_params["tables"]:
        with TaskGroup(group_id=f"{table['schema']}_{table['table']}_pipeline") as tg:

            run_sql_to_gcs = DataflowStartFlexTemplateOperator(
                task_id=f"run_sql_to_gcs_{table['schema']}_{table['table']}_batched",
                project_id=env_config["project_id"],
                location=env_config["region"],
                body={
                    "launchParameter": {
                        "jobName": f"sql-gcs-{table['schema']}-{table['table']}-batched",
                        "containerSpecGcsPath": env_config["template_sql_to_gcs_batched"],
                        "environment": {
                            "serviceAccountEmail": env_config["dataflow_service_account"],
                            "subnetwork": env_config["subnetwork"],
                            "tempLocation": env_config["dataflow_temp_location"],
                            "stagingLocation": env_config["dataflow_staging_location"],
                            "ipConfiguration": "WORKER_IP_PRIVATE"
                        },
                        "parameters": {
                            "gcp_project": env_config["project_id"],
                            "schema": table['schema'],
                            "table" : table['table'],
                            "reload_flag": str(table.get("reload_flag") or "false").lower(),
                            "type_of_extraction" : table["type_of_extraction"],
                            "batch_size": str(table['batch_size']),
                            "output_gcs_path": table["output_gcs_path"],
                            "query": table["query"],
                            "secret_id": table["secret_id"],
                            "primary_key": ",".join(table.get("primary_key", [])),
                            "num_workers" : str(table['num_workers']),
                            "max_num_workers" : str(table['max_num_workers']),
                            "autoscaling_algorithm" : "THROUGHPUT_BASED",
                            "machine_type" : "e2-highmem-4",
                            "chunk_size" : str(table['chunk_size'])
                            
                        }
                    }
                }
            )
            
            copy_raw_parquet_files_to_processed_bucket = GCSToGCSOperator(
                task_id=f"copy_gcs_to_processed_{table['schema']}_{table['table']}",
                source_bucket=table["output_gcs_path"].replace("gs://", "").split("/")[0],
                source_object="/".join(table["output_gcs_path"].replace("gs://", "").split("/")[1:]) + "*",
                destination_bucket=table["gcs_external_table_path"].replace("gs://", "").split("/")[0],
                destination_object="/".join(table["gcs_external_table_path"].replace("gs://", "").split("/")[1:]),
                move_object=False
            )
            create_external_table = BigQueryInsertJobOperator(
                task_id=f"create_external_table_{table['schema']}_{table['table']}",
                project_id=env_config["project_id"],
                location=env_config["region"],
                configuration={
                    "query": {
                        "query": f"""
                        CREATE OR REPLACE EXTERNAL TABLE `{env_config["project_id"]}.sqlserver_to_bq_bronze.{table['database']}_{table['schema']}_{table['table']}`
                        OPTIONS (
                            format = 'PARQUET',
                            uris = ['{table['gcs_external_table_path']}*']
                        );
                        """,
                        "useLegacySql": False,
                    }
                }
            )
            #---------------------
            #consolidate table versions from gcs processed bucket
            #---------------------
            generate_ddl = PythonOperator(
                task_id=f"generate_ddl_{table['schema']}_{table['table']}",
                python_callable=lambda **kwargs: build_iceberg_sql(
                env_config["project_id"], env_config["dataops_dataset"], env_config["table_extraction_metadata"], table['schema'], table['table'], table['primary_keys'], table['database'], env_config["region"], table['server'], kwargs
                    )
                )

            check_iceberg = BigQueryInsertJobOperator(
                task_id=f"check_iceberg_table_{table['schema']}_{table['table']}",
                project_id=env_config["project_id"],
                location=env_config["region"],
                configuration={
                    "query": {
                        "query": f"""
                            SELECT table_name
                            FROM `{env_config["project_id"]}.sqlserver_to_bq_silver.INFORMATION_SCHEMA.TABLES`
                            WHERE table_name = '{table['table']}'
                        """,
                        "useLegacySql": False
                    }
                }
            )


            def branch_create_or_insert(**kwargs):
                result = kwargs["ti"].xcom_pull(task_ids=f"check_iceberg_table_{table['schema']}_{table['table']}")
                return f"create_iceberg_{table['schema']}_{table['table']}" if not result else f"insert_iceberg_{table['schema']}_{table['table']}"

            branch_task = BranchPythonOperator(
                task_id=f"branch_{table['schema']}_{table['table']}",
                python_callable=branch_create_or_insert
            )
            
            
            create_iceberg = BigQueryInsertJobOperator(
                task_id=f"create_iceberg_{table['schema']}_{table['table']}",
                project_id=env_config["project_id"],
                location=env_config["region"],
                configuration={
                    "query": {
                        "query": "{{ ti.xcom_pull(task_ids='" + f"generate_ddl_{table['schema']}_{table['table']}" + "', key='iceberg_ddl') }}",
                        "useLegacySql": False
                    }
                }
            )
            
            
            insert_iceberg = BigQueryInsertJobOperator(
                task_id=f"insert_iceberg_{table['schema']}_{table['table']}",
                project_id=env_config["project_id"],
                location=env_config["region"],
                configuration={
                    "query": {
                        "query": f"""
                            INSERT INTO `{env_config["project_id"]}.sqlserver_to_bq_silver.{table['schema']}_{table['table']}`
                            SELECT * FROM `{env_config["project_id"]}.sqlserver_to_bq_bronze.{table['schema']}_{table['table']}_ext`
                        """,
                        "useLegacySql": False
                    }
                }
            )


            run_sql_to_gcs >> copy_raw_parquet_files_to_processed_bucket >> create_external_table >> create_iceberg >> insert_iceberg

        task_groups.append(tg)

    log_success = PythonOperator(
        task_id="log_success",
        python_callable=lambda **context: print("DAG completed successfully")
    )

    log_start >> task_groups >> log_success
